{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Data"
      ],
      "metadata": {
        "id": "XDCnONVSN5Z1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtOroiAUBax4",
        "outputId": "79c1930b-943e-4f22-8205-01a83aedbe8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "All Files Count in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/ : 0\n",
            "--------------------------------------------------\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Cats/ Before Augmentation : 0\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Dogs/ Before Augmentation : 0\n",
            "-------------------------------------------------\n",
            "Number of files for testing model in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/ : 0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/'\n",
        "\n",
        "file_path  = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/'\n",
        "file1_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Cats/'\n",
        "file2_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Dogs/'\n",
        "\n",
        "test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "all_files = [f for f in os.listdir(file_path) if os.path.isfile(os.path.join(file_path, f))]\n",
        "\n",
        "files1 = [f for f in os.listdir(file1_path) if os.path.isfile(os.path.join(file1_path, f))]\n",
        "files2 = [f for f in os.listdir(file2_path) if os.path.isfile(os.path.join(file2_path, f))]\n",
        "\n",
        "test_files = [f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))]\n",
        "\n",
        "\n",
        "print(f'All Files Count in {file_path} : {len(all_files)}')\n",
        "print('--------------------------------------------------')\n",
        "print(f\"Number of files in {file1_path} Before Augmentation : {len(files1)}\")\n",
        "print(f\"Number of files in {file2_path} Before Augmentation : {len(files2)}\")\n",
        "print('-------------------------------------------------')\n",
        "print(f'Number of files for testing model in {test_path} : {len(test_files)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepocessing images"
      ],
      "metadata": {
        "id": "IqPa5xHsZZHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "file_path  = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/'\n",
        "file1_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Cats'\n",
        "file2_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Dogs'\n",
        "\n",
        "test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "save_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/'\n",
        "\n",
        "# Define the augmentation parameters\n",
        "rotation_angle = 30\n",
        "zoom_range = (0.75, 1.25)\n",
        "\n",
        "# Function to perform image augmentation\n",
        "def augment_image(image_path , save_path):\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Horizontal flipping\n",
        "    flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    flipped_image.save(os.path.join(save_path, f\"flipped_{os.path.basename(image_path)}\"))\n",
        "\n",
        "    # Rotation\n",
        "    angle = random.randint(-rotation_angle, rotation_angle)\n",
        "    rotated_image = image.rotate(angle)\n",
        "    rotated_image.save(os.path.join(save_path, f\"rotated_{os.path.basename(image_path)}\"))\n",
        "\n",
        "    # Scaling or zooming\n",
        "    zoom_factor = random.uniform(zoom_range[0], zoom_range[1])\n",
        "    scaled_image = image.resize((int(image.width * zoom_factor), int(image.height * zoom_factor)))\n",
        "    scaled_image.save(os.path.join(save_path, f\"scaled_{os.path.basename(image_path)}\"))\n",
        "\n",
        "\n",
        "# Iterate over all image files in the directory\n",
        "for file_name in os.listdir(file1_path):\n",
        "    if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
        "        file_path = os.path.join(file1_path, file_name)\n",
        "        augment_image(file_path, save_path)\n",
        "\n",
        "# Iterate over all image files in the directory\n",
        "for file_name in os.listdir(file2_path):\n",
        "    if file_name.endswith(\".jpg\") or file_name.endswith(\".png\"):\n",
        "        file_path = os.path.join(file2_path, file_name)\n",
        "        augment_image(file_path, save_path)\n",
        "\n",
        "files1 = [f for f in os.listdir(file1_path) if os.path.isfile(os.path.join(file1_path, f))]\n",
        "files2 = [f for f in os.listdir(file2_path) if os.path.isfile(os.path.join(file2_path, f))]\n",
        "\n",
        "augmented_files = [f for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path , f))]\n",
        "\n",
        "print(f\"Number of files in {file1_path} After Augmentation : {len(files1)}\")\n",
        "print(f\"Number of files in {file2_path} After Augmentation : {len(files2)}\")\n",
        "print('-----------------------------------------------')\n",
        "\n",
        "print(f\"Number of files in {save_path} After Augmentation : {len(augmented_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV-hGT7zZYt7",
        "outputId": "3dba673c-ea91-49d2-9eec-7b6421efad5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Cats After Augmentation : 350\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/Dogs After Augmentation : 352\n",
            "-----------------------------------------------\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_After_Augment/ After Augmentation : 2106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet Model"
      ],
      "metadata": {
        "id": "Ft0a3hkZpoqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define hyperparameters\n",
        "initial_lr = 0.001\n",
        "lr_decay_rate = 0.1\n",
        "momentum = 0.9\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Replace the FC layers with new ones for binary classification (dog vs cat)\n",
        "x = Flatten()(base_model.output)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze all layers except the new FC layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=initial_lr, momentum=momentum)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation for training images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   validation_split=0.2)  # Splitting the dataset into training and validation\n",
        "# all_train_path = ''\n",
        "all_train_path  = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(all_train_path,\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='training')  # Training set\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(all_train_path,\n",
        "                                                         target_size=(224, 224),\n",
        "                                                         batch_size=batch_size,\n",
        "                                                         class_mode='categorical',\n",
        "                                                         subset='validation')  # Validation set\n",
        "\n",
        "# Train the model with training and validation data\n",
        "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "\n",
        "# Unfreeze the last CONV block\n",
        "for layer in model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile the model to apply the changes\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# all_test_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Test/'\n",
        "all_test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "# Test data generator for evaluation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(all_test_path ,\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoOo3xx_pqnF",
        "outputId": "7208d5cf-4391-486e-bcd0-e6ae976490f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n",
            "Found 562 images belonging to 2 classes.\n",
            "Found 140 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "18/18 [==============================] - 268s 15s/step - loss: 27.5855 - accuracy: 0.5338 - val_loss: 21.0979 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 155s 9s/step - loss: 15.5415 - accuracy: 0.5053 - val_loss: 20.0470 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 147s 8s/step - loss: 9.3486 - accuracy: 0.5854 - val_loss: 53.4463 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 145s 8s/step - loss: 18.7121 - accuracy: 0.5320 - val_loss: 24.1719 - val_accuracy: 0.5071\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 156s 9s/step - loss: 11.5762 - accuracy: 0.5676 - val_loss: 18.4106 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 142s 8s/step - loss: 9.3950 - accuracy: 0.5801 - val_loss: 3.7200 - val_accuracy: 0.5500\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 157s 9s/step - loss: 9.9327 - accuracy: 0.5712 - val_loss: 7.1422 - val_accuracy: 0.5571\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 143s 8s/step - loss: 26.2031 - accuracy: 0.4858 - val_loss: 3.4521 - val_accuracy: 0.6571\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 141s 8s/step - loss: 24.8649 - accuracy: 0.5107 - val_loss: 29.2694 - val_accuracy: 0.5000\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 142s 8s/step - loss: 17.4492 - accuracy: 0.5480 - val_loss: 17.7246 - val_accuracy: 0.5000\n",
            "Found 100 images belonging to 2 classes.\n",
            "4/4 [==============================] - 71s 21s/step - loss: 21.5642 - accuracy: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[21.56424331665039, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16"
      ],
      "metadata": {
        "id": "4GYYd5M0KUNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define hyperparameters\n",
        "initial_lr = 0.1\n",
        "lr_decay_rate = 0.002\n",
        "momentum = 0.9\n",
        "batch_size = 10\n",
        "epochs = 50\n",
        "\n",
        "# Load pre-trained VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Replace the FC layers with new ones for binary classification (dog vs cat)\n",
        "x = Flatten()(base_model.output)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze all layers except the new FC layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=initial_lr, momentum=momentum)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation for training images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   validation_split=0.2)  # Splitting the dataset into training and validation\n",
        "\n",
        "\n",
        "all_train_path  = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Train/'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(all_train_path,\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='training')  # Training set\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(all_train_path,\n",
        "                                                         target_size=(224, 224),\n",
        "                                                         batch_size=batch_size,\n",
        "                                                         class_mode='categorical',\n",
        "                                                         subset='validation')  # Validation set\n",
        "\n",
        "# Train the model with training and validation data\n",
        "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "\n",
        "# Unfreeze the last CONV block\n",
        "for layer in model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile the model to apply the changes\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "all_test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "# Test data generator for evaluation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(all_test_path ,\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "2ufbYo3OKWd6",
        "outputId": "1b977091-859f-4e8c-e9a0-947436e250cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images belonging to 2 classes.\n",
            "Found 0 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a331c3937e58>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Train the model with training and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Unfreeze the last CONV block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/preprocessing/image.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\"Asked to retrieve element {idx}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;34m\"but the Sequence \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
          ]
        }
      ]
    }
  ]
}