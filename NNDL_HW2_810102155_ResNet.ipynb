{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Data"
      ],
      "metadata": {
        "id": "6OGcTuijcQS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ml8MVMzeRG",
        "outputId": "b7556384-55c0-4416-c7d8-88d5b65b2fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_After_Augment/ After Augmentation : 2166\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/ Before Augmentation : 0\n",
            "-----------------------------------------------\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/ : 0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/'\n",
        "# file1_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/Cats'\n",
        "# file2_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/Dogs'\n",
        "\n",
        "# train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/'\n",
        "train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/'\n",
        "\n",
        "# test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augment/HW2_Dataset/Test/'\n",
        "test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/cats'\n",
        "test_path2 = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/dogs'\n",
        "\n",
        "files1 = [f for f in os.listdir(train_path) if os.path.isfile(os.path.join(train_path, f))]\n",
        "files2 = [f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))]\n",
        "\n",
        "test_files = [f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))]\n",
        "\n",
        "print(f\"Number of files in {train_path} After Augmentation : {len(files1)}\")\n",
        "print(f\"Number of files in {test_path} Before Augmentation : {len(files2)}\")\n",
        "print('-----------------------------------------------')\n",
        "print(f'Number of files in {test_path} : {len(test_files)}')\n",
        "\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "x4HvVqDgzSV5",
        "outputId": "aa7130e3-c5f3-4a80-8e8f-876549a141e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bae9267ada33>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m train_generator = train_datagen.flow_from_directory(file_path,\n\u001b[0m\u001b[1;32m     41\u001b[0m                                                     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define hyperparameters\n",
        "initial_lr = 0.001\n",
        "lr_decay_rate = 0.1\n",
        "momentum = 0.9\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Replace the FC layers with new ones for binary classification (dog vs cat)\n",
        "x = Flatten()(base_model.output)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze all layers except the new FC layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=initial_lr, momentum=momentum)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation for training images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   validation_split=0.2)  # Splitting the dataset into training and validation\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(file_path,\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='training')  # Training set\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(file_path,\n",
        "                                                         target_size=(224, 224),\n",
        "                                                         batch_size=batch_size,\n",
        "                                                         class_mode='categorical',\n",
        "                                                         subset='validation')  # Validation set\n",
        "\n",
        "# Train the model with training and validation data\n",
        "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "\n",
        "# Unfreeze the last CONV block\n",
        "for layer in model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile the model to apply the changes\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "all_test_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Test/'\n",
        "\n",
        "# Test data generator for evaluation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(all_test_path ,\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(test_generator)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}