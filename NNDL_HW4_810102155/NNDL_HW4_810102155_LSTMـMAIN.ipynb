{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use ParsBert"
   ],
   "metadata": {
    "id": "NiQIWU7a57xH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # # Install necessary libraries\n",
    "!pip install transformers\n",
    "!pip install hazm"
   ],
   "metadata": {
    "id": "McC7UXOf56cU"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from hazm import Normalizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUXfJEK76Bo4",
    "outputId": "2ff97d68-0a8b-47b0-c554-9ae950215fc2"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive' , force_remount=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path1 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/anger.csv'\n",
    "file_path2 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/disgust.csv'\n",
    "file_path3 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/fear.csv'\n",
    "\n",
    "file_path4 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/joy.csv'\n",
    "file_path5 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/sad.csv'\n",
    "file_path6 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/surprise.csv'\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "df3 = pd.read_csv(file_path3)\n",
    "\n",
    "df4 = pd.read_csv(file_path4)\n",
    "df5 = pd.read_csv(file_path5)\n",
    "df6 = pd.read_csv(file_path6)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index = True)\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop = True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDFAic3i6Lxx",
    "outputId": "23ca396f-e562-40da-cc18-993ed40b28b0"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "stopwords = ['Ùˆ', 'Ø¯Ø±', 'Ø¨Ù‡' ,\n",
    "             'Ø§Ø²' , 'Ú©Ù‡' , 'Ø§ÛŒÙ†' , 'Ø§Ø³Øª',\n",
    "             'Ø§ÛŒÙ†' , 'Ù…ÛŒ' , 'Ø±Ø§' , 'Ø¨Ø§' , 'Ù‡Ø§ÛŒ' , 'Ø¨Ø±Ø§ÛŒ' , 'Ø¢Ù†' , 'ÛŒÚ©' ,\n",
    "             'Ø¨Ø±Ø§ÛŒ' , 'Ù‡Ø§ÛŒ' ,\n",
    "             'Ø®ÙˆØ¯' , 'Ù‡Ø§' , 'Ú©Ø±Ø¯' , 'Ø´Ø¯' , 'Ø§ÛŒ' , 'ØªØ§' ,\n",
    "             'Ú©Ù†Ø¯' , 'Ø¨Ø±' , 'Ø¨ÙˆØ¯' , 'Ú¯ÙØª' ,\n",
    "             'Ù†ÛŒØ²' , 'ÙˆÛŒ' , 'Ù‡Ù…' , 'Ú©Ù†Ù†Ø¯' , 'Ø¯Ø§Ø±Ø¯' , 'Ù…Ø§' , 'Ú©Ø±Ø¯Ù‡' ,\n",
    "             'ÛŒØ§' , 'Ø§Ù…Ø§' , 'Ø¨Ø§ÛŒØ¯' ,\n",
    "             'Ø¯Ùˆ' , 'Ø§Ù†Ø¯' , 'Ù‡Ø±' , 'Ø®ÙˆØ§Ù‡Ø¯' , 'Ø§Ùˆ' , 'Ù…ÙˆØ±Ø¯' , 'Ø¨Ø§Ø´Ø¯' ,\n",
    "             'Ø¯ÛŒÚ¯Ø±' , 'Ø¨ÛŒÙ†' , 'Ù¾ÛŒØ´' ,\n",
    "             'Ù¾Ø³' , 'Ø§Ú¯Ø±' , 'Ù‡Ù…Ù‡' , 'ØµÙˆØ±Øª' , 'ÛŒÚ©ÛŒ' , 'Ù‡Ø³ØªÙ†Ø¯' , 'Ù…Ù†' , 'Ø¯Ù‡Ø¯' ,\n",
    "             'Ù†ÛŒØ³Øª' , 'Ø§Ø³ØªÙØ§Ø¯Ù‡' ,\n",
    "             'Ø¯Ø§Ø¯' , 'Ø¯Ø§Ø´ØªÙ‡' , 'Ø±Ø§Ù‡' , 'Ø¯Ø§Ø´Øª' , 'Ú†Ù‡' , 'Ù‡Ù…Ú†Ù†ÛŒÙ†' ,\n",
    "             'Ú©Ø±Ø¯Ù†Ø¯' , 'Ø¯Ø§Ø¯Ù‡' , 'Ø¨ÙˆØ¯Ù‡' , 'Ø¯Ø§Ø±Ù†Ø¯' ,\n",
    "             'Ù‡Ù…ÛŒÙ†' , 'Ø³ÙˆÛŒ' , 'Ø´ÙˆÙ†Ø¯' , 'Ø¨Ø³ÛŒØ§Ø±' , 'Ø±ÙˆÛŒ' , 'Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯' ,\n",
    "             'Ù‡Ø§ÛŒÛŒ' , 'ØªÙˆØ§Ù†Ø¯' , 'Ø­ØªÛŒ' , 'Ø§ÛŒÙ†Ú©Ù‡' ,\n",
    "             'Ø§ÛŒÙ†â€ŒÚ©Ù‡' , 'ÙˆÙ„ÛŒ' , 'ØªÙˆØ³Ø·' , 'Ú†Ù†ÛŒÙ†' , 'Ø¨Ø±Ø®ÛŒ' ,\n",
    "             'Ø¯Ø±Ø¨Ø§Ø±Ù‡' , 'Ú¯ÛŒØ±Ø¯' , 'Ú¯ÙØªÙ‡' , 'Ø¢Ù†Ø§Ù†' ,\n",
    "             'Ø¨Ø§Ø±' , 'Ø·ÙˆØ±' , 'Ú¯Ø±ÙØª' , 'Ø¯Ù‡Ù†Ø¯' , 'Ú¯Ø°Ø§Ø±ÛŒ' , 'Ø¨Ø³ÛŒØ§Ø±ÛŒ' ,\n",
    "             'Ø·ÛŒ' , 'Ø¨ÙˆØ¯Ù†Ø¯' , 'Ø¨Ø±Ø§Ø³Ø§Ø³' , 'Ø´Ø¯Ù†Ø¯' ,\n",
    "             'Ø¨Ø§Ø´Ù†Ø¯' , 'Ú†ÙˆÙ†' , 'Ù‚Ø§Ø¨Ù„' , 'Ú¯ÙˆÛŒØ¯' , 'Ø¯ÛŒÚ¯Ø±ÛŒ' , 'Ù‡Ù…Ø§Ù†' , 'Ø®ÙˆØ§Ù‡Ù†Ø¯' ,\n",
    "             'Ø·Ø±ÛŒÙ‚' , 'Ø¢Ù…Ø¯Ù‡' , 'ØªØ­Øª' ,\n",
    "             'Ú¯ÛŒØ±ÛŒ' , 'Ø¬Ø§ÛŒ' , 'Ø³Ø§Ø²ÛŒ' , 'Ú©Ù†Ù…' , 'Ø²ÛŒØ±' , 'ØªÙˆØ§Ù†Ù†Ø¯' , 'Ø¶Ù…Ù†' ,\n",
    "             'ÙÙ‚Ø·' , 'Ø¨ÙˆØ¯Ù†' , 'Ø¢ÛŒØ¯' ,\n",
    "             'Ø§Ø´' , 'Ø§Ù…' , 'Ø§Øª' , 'Ø¢ÙˆØ±Ø¯' , 'Ø§Ù…Ø§Ù†' , 'Ø§ØªØ§Ù†' , 'Ø§Ø´Ø§Ù†' ,\n",
    "             'Ø¢Ù†Ú†Ù‡' , 'Ø±ÛŒØ²ÛŒ' ,\n",
    "             'Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†' , 'Ø¨Ø¹Ø¶ÛŒ' , 'Ø¨Ø±Ø®ÛŒ' , 'Ø¯Ø§Ø¯Ù†Ø¯' , 'Ø¯Ø§Ø´ØªÙ†Ø¯' ,\n",
    "             'Ø²ÛŒØ±' , 'Ø±ÙˆÛŒ' , 'Ø³Ø±ÛŒ' , 'ØªÙˆÛŒ' , 'Ø¬Ù„ÙˆÛŒ' ,\n",
    "             'Ù¾ÛŒØ´' , 'Ø¹Ù‚Ø¨' , 'Ø¨Ø§Ù„Ø§ÛŒ' , 'Ø®Ø§Ø±Ø¬' , 'ÙˆØ³Ø·' , 'Ø¨ÛŒØ±ÙˆÙ†' , 'Ø³ÙˆÛŒ' , 'Ú©Ù†Ø§Ø±' ,\n",
    "             'Ù†Ø²Ø¯' , 'Ø¯Ù†Ø¨Ø§Ù„' ,\n",
    "             'Ø­Ø¯ÙˆØ¯' , 'Ø¨Ø±Ø§Ø¨Ø±' , 'Ø§Ø«Ø±' , 'Ø¹Ù„Øª' , 'Ø¹Ù†ÙˆØ§Ù†' , 'Ù‚ØµØ¯' , 'Ø¬Ø¯Ø§' ,\n",
    "             'Ú©ÛŒ' , 'Ú©Ù‡' , 'Ú†ÛŒØ³Øª' ,\n",
    "             'Ù‡Ø³Øª' , 'Ú©Ø¬Ø§' , 'Ú©Ø¬Ø§Ø³Øª' , 'Ú†Ø·ÙˆØ±' , 'Ú©Ø¯Ø§Ù…' , 'Ø¢ÛŒØ§' , 'Ù…Ú¯Ø±' ,\n",
    "             'Ú†Ù†Ø¯ÛŒÙ†' , 'ÛŒÚ©' , 'Ú†ÛŒØ²ÛŒ' ,\n",
    "             'Ø¯ÛŒÚ¯Ø±' , 'Ú©Ø³ÛŒ' , 'Ú†ÛŒØ²' , 'Ø¬Ø§' , 'Ú©Ø³' , 'Ù„Ø·ÙØ§' , 'ØªØ§Ù†' , 'Ù…Ø§Ù†' ,\n",
    "             'Ù‡Ù†Ú¯Ø§Ù…ÛŒ' , 'ÙˆÙ‚ØªÛŒÚ©Ù‡' ,\n",
    "             'Ù…Ø¯ØªÛŒ' ,\n",
    "             'Ø¢Ù†Ú©Ù‡' , 'Ø§Ù†Ú©Ù‡' , 'Ø®ÙˆØ§Ù‡Ø´Ø§' , 'ÙˆÙ‚ØªÛŒÚ©Ù‡'\n",
    "             ]\n",
    "\n",
    "# print('----------- length of stopwords ---------------')\n",
    "# print(len(stopwords))\n",
    "# print('-----------------------------------------------')\n",
    "# print('Ù…Ø¯ØªÛŒ' in stopwords)"
   ],
   "metadata": {
    "id": "DdFeDx016Olk"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "emoji_mapping = {\n",
    "    'ğŸ˜Š': 'Ø®Ù†Ø¯Ø§Ù†',\n",
    "    'ğŸ˜¢': 'ØºÙ…Ú¯ÛŒÙ†',\n",
    "    'ğŸ˜‚': 'Ø®Ù†Ø¯Ù‡',\n",
    "    'ğŸ˜': 'Ø¹Ø§Ø´Ù‚',\n",
    "    'â¤ï¸': 'Ø¹Ø´Ù‚',\n",
    "    'ğŸ‘': 'Ø®ÙˆØ¨',\n",
    "    'ğŸ‘': 'Ø¨Ø¯',\n",
    "    'ğŸ™': 'Ø¯Ø¹Ø§',\n",
    "    'ğŸ”¥': 'Ø¢ØªØ´',\n",
    "    'ğŸ”´' : 'Ù‚Ø±Ù…Ø²' ,\n",
    "    'ğŸŸ¡' : 'Ø²Ø±Ø¯' ,\n",
    "    'âœŠ' : 'Ù…Ø´Øª' ,\n",
    "    'ğŸ’¥' : 'Ù‡ÛŒØ¬Ø§Ù†' ,\n",
    "    'ğŸ“¡' : 'ÙÙˆØ±ÛŒ' ,\n",
    "    'â¬…ï¸' : 'ØªÙ…Ø§Ø´Ø§' ,\n",
    "    'ğŸ‡®ğŸ‡·' : 'Ø§ÛŒØ±Ø§Ù†' ,\n",
    "    'âœŒ' : 'Ø¯ÙˆØ¨Ø§Ø±Ù‡' ,\n",
    "    'ğŸ’”': 'Ø´Ú©Ø³ØªÙ‡',\n",
    "    'ğŸŒ¸': 'Ú¯Ù„',\n",
    "    'ğŸŒ': 'Ø®ÙˆØ±Ø´ÛŒØ¯',\n",
    "    'ğŸŒˆ': 'Ø±Ù†Ú¯ÛŒÙ†â€ŒÚ©Ù…Ø§Ù†',\n",
    "    'ğŸ‰': 'ØªØ¨Ø±ÛŒÚ©',\n",
    "    'ğŸ¤—': 'Ø¯ÙˆØ³ØªØ§Ù†Ù‡',\n",
    "    'ğŸ¤”': 'ØªÙÚ©Ø±',\n",
    "    'ğŸ˜œ': 'Ø´ÙˆØ®ÛŒ'\n",
    "}"
   ],
   "metadata": {
    "id": "b1bDvXTt6S5r"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing tweets"
   ],
   "metadata": {
    "id": "Thh0Ic1h6XLd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from hazm import Normalizer, Stemmer, word_tokenize\n",
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet , stop_words , emoji_mapping):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|<\\S+>', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "\n",
    "    tweet = normalizer.normalize(tweet)\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "    for emoji, text in emoji_mapping.items():\n",
    "        tweet = tweet.replace(emoji, text)\n",
    "\n",
    "    stemmed_words = [word for word in words]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "df['preprocessed_tweet'] = df['tweet'].apply(lambda x: preprocess_tweet(x, stopwords, emoji_mapping))\n",
    "\n",
    "# df.sample(7)[['hashtags' , 'tweet', 'emotion', 'preprocessed_tweet']]"
   ],
   "metadata": {
    "id": "cHvLiudq6eFS"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {
    "id": "eZHEQ9ob6kmg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "def tokenize_and_pad(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"]\n",
    "\n",
    "\n",
    "df['ids'] = df['preprocessed_tweet'].apply(tokenize_and_pad)\n",
    "# df.sample(7)[['hashtags' , 'tweet', 'emotion', 'preprocessed_tweet' , 'ids']]"
   ],
   "metadata": {
    "id": "tYfmHPYz6m_w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9992293-f471-4839-8b06-cb59895c7faf"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset"
   ],
   "metadata": {
    "id": "HbSzYojj6rzL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "id": "s8QvjYT86uWY"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "emotion_to_label = {\n",
    "    'anger': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'joy': 3,\n",
    "    'sad': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['preprocessed_tweet']\n",
    "        emotion = self.data.iloc[idx]['emotion']\n",
    "\n",
    "        label = emotion_to_label[emotion]\n",
    "        tokens = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "\n",
    "        input_ids = tokens\n",
    "        # padding_length = self.max_length - input_ids.shape[1]\n",
    "        # if padding_length > 0:\n",
    "        #     input_ids = torch.cat([input_ids, torch.zeros((1, padding_length), dtype=torch.long)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text' : text ,\n",
    "            'input_ids': input_ids,\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer , max_length=32)\n",
    "val_dataset   = CustomDataset(val_data  , tokenizer , max_length=32)\n",
    "test_dataset  = CustomDataset(test_data,  tokenizer , max_length=32)"
   ],
   "metadata": {
    "id": "uO33oh5L6y0x",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b754cf6d-f5c7-4a95-cc6d-cace47bed60d"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM Model"
   ],
   "metadata": {
    "id": "XbxLnMNr6aUz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ids = train_data['preprocessed_tweet'].apply(tokenize_and_pad)\n",
    "\n",
    "vocab_size = len(ids)\n",
    "\n",
    "print(f'vocab_size  : {vocab_size}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znmIwyma_Yjg",
    "outputId": "c9527595-22ed-48a7-9359-30ad19ffe7c9"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab_size  : 79680\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_MuSga4dTGQl"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers , bidirectional, dropout):\n",
    "#         super(LSTM, self).__init__()\n",
    "\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "#     def forward(self, text):\n",
    "#         text = torch.where(text < self.vocab_size , text , torch.tensor( self.vocab_size - 1 ))\n",
    "#         embedded = self.embedding(text)\n",
    "#         # text shape: [seq_len, batch_size]\n",
    "\n",
    "#         output, (hidden, cell) = self.lstm(embedded)  # output shape: [seq_len, batch_size, hidden_dim * num_directions]\n",
    "\n",
    "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))  # Concatenating the final hidden state from both directions\n",
    "\n",
    "#         output = self.fc(hidden)\n",
    "\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.5, pad_idx=0):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # Initialize the LSTM layer with bidirectional set to True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        # Since the LSTM is bidirectional, the output dimensions will be doubled\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        clamped_text = torch.clamp(text, 0, self.vocab_size - 1)\n",
    "\n",
    "        # text is [batch size, sent len]\n",
    "        embedded = self.embedding(clamped_text)\n",
    "        # embedded is [batch size, sent len, emb dim]\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        dropped = self.dropout(hidden)\n",
    "\n",
    "\n",
    "        output = self.fc(dropped)\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "id": "6bOF9JYrGRD8"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finding Optimal Hyperparameters"
   ],
   "metadata": {
    "id": "E_EigGg-_DQs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'vocab_size  : {vocab_size}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgMJZbENDTeb",
    "outputId": "860e39e3-a7a9-44f6-ef45-5fd2eac4ad74"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab_size  : 79680\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# hyperparameters\n",
    "batch_sizes = [64, 8]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "optimizers = {'Adam': Adam, 'SGD': SGD}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for batch in train_loader:\n",
    "            data = batch['input_ids']['input_ids'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "\n",
    "            data = data.squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print('-----------------------------------------------')\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                data = batch['input_ids']['input_ids'].to(device)\n",
    "                targets = batch['label'].to(device)\n",
    "                data = data.squeeze(1)\n",
    "                outputs = model(data)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        print()\n",
    "\n",
    "\n",
    "model = LSTM_Model(vocab_size, embedding_dim = 120 , hidden_dim = 6 , output_dim = 6 , num_layers = 2 ,  dropout = 0.5 )\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for opt_name, Optimizer in optimizers.items():\n",
    "            print(f'Testing batch size {batch_size}, learning ratne {lr}, optimizer {opt_name}')\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            optimizer = Optimizer(model.parameters(), lr=lr)\n",
    "            train_model(model, criterion, optimizer, train_loader, val_loader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QK0ndrpz-aJo",
    "outputId": "6c8ca132-14fc-4262-ddde-4123678f0ebd"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing batch size 64, learning ratne 0.001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 1.4965, Train Accuracy: 35.15%\n",
      "Epoch 1, Validation Loss: 1.2640, Validation Accuracy: 48.33%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 1.1773, Train Accuracy: 52.39%\n",
      "Epoch 2, Validation Loss: 1.0773, Validation Accuracy: 56.62%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 1.0231, Train Accuracy: 60.49%\n",
      "Epoch 3, Validation Loss: 0.9830, Validation Accuracy: 63.53%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.9188, Train Accuracy: 65.66%\n",
      "Epoch 4, Validation Loss: 0.9472, Validation Accuracy: 65.65%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.8614, Train Accuracy: 67.60%\n",
      "Epoch 5, Validation Loss: 0.9352, Validation Accuracy: 67.70%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.8171, Train Accuracy: 69.68%\n",
      "Epoch 6, Validation Loss: 0.9251, Validation Accuracy: 67.97%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.7822, Train Accuracy: 70.69%\n",
      "Epoch 7, Validation Loss: 0.9337, Validation Accuracy: 67.85%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.7440, Train Accuracy: 72.18%\n",
      "Epoch 8, Validation Loss: 0.9197, Validation Accuracy: 68.95%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.7205, Train Accuracy: 72.66%\n",
      "Epoch 9, Validation Loss: 0.9250, Validation Accuracy: 68.60%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.6800, Train Accuracy: 74.32%\n",
      "Epoch 10, Validation Loss: 0.8824, Validation Accuracy: 70.30%\n",
      "\n",
      "Testing batch size 64, learning ratne 0.001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.6381, Train Accuracy: 75.75%\n",
      "Epoch 1, Validation Loss: 0.8977, Validation Accuracy: 69.79%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.6353, Train Accuracy: 75.82%\n",
      "Epoch 2, Validation Loss: 0.8872, Validation Accuracy: 70.27%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.6346, Train Accuracy: 75.81%\n",
      "Epoch 3, Validation Loss: 0.8912, Validation Accuracy: 70.25%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.6329, Train Accuracy: 75.79%\n",
      "Epoch 4, Validation Loss: 0.8861, Validation Accuracy: 70.33%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.6330, Train Accuracy: 75.86%\n",
      "Epoch 5, Validation Loss: 0.8891, Validation Accuracy: 70.42%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.6327, Train Accuracy: 76.05%\n",
      "Epoch 6, Validation Loss: 0.8878, Validation Accuracy: 70.27%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.6326, Train Accuracy: 75.92%\n",
      "Epoch 7, Validation Loss: 0.8908, Validation Accuracy: 70.28%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.6284, Train Accuracy: 75.88%\n",
      "Epoch 8, Validation Loss: 0.8869, Validation Accuracy: 70.32%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.6261, Train Accuracy: 75.98%\n",
      "Epoch 9, Validation Loss: 0.8906, Validation Accuracy: 70.42%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.6254, Train Accuracy: 75.93%\n",
      "Epoch 10, Validation Loss: 0.8907, Validation Accuracy: 70.38%\n",
      "\n",
      "Testing batch size 64, learning ratne 0.0001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.6244, Train Accuracy: 76.15%\n",
      "Epoch 1, Validation Loss: 0.8919, Validation Accuracy: 70.57%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.6157, Train Accuracy: 76.42%\n",
      "Epoch 2, Validation Loss: 0.8882, Validation Accuracy: 70.58%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.6095, Train Accuracy: 76.49%\n",
      "Epoch 3, Validation Loss: 0.8956, Validation Accuracy: 70.61%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.6008, Train Accuracy: 76.80%\n",
      "Epoch 4, Validation Loss: 0.8904, Validation Accuracy: 70.48%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.5955, Train Accuracy: 76.87%\n",
      "Epoch 5, Validation Loss: 0.8984, Validation Accuracy: 70.56%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.5889, Train Accuracy: 77.03%\n",
      "Epoch 6, Validation Loss: 0.9005, Validation Accuracy: 70.68%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.5821, Train Accuracy: 77.26%\n",
      "Epoch 7, Validation Loss: 0.8997, Validation Accuracy: 70.78%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.5720, Train Accuracy: 77.52%\n",
      "Epoch 8, Validation Loss: 0.8963, Validation Accuracy: 70.83%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.5629, Train Accuracy: 77.77%\n",
      "Epoch 9, Validation Loss: 0.8985, Validation Accuracy: 70.94%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.5609, Train Accuracy: 77.81%\n",
      "Epoch 10, Validation Loss: 0.8992, Validation Accuracy: 70.95%\n",
      "\n",
      "Testing batch size 64, learning ratne 0.0001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.5495, Train Accuracy: 78.18%\n",
      "Epoch 1, Validation Loss: 0.8997, Validation Accuracy: 71.03%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.5513, Train Accuracy: 78.10%\n",
      "Epoch 2, Validation Loss: 0.8997, Validation Accuracy: 71.03%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.5504, Train Accuracy: 78.08%\n",
      "Epoch 3, Validation Loss: 0.9001, Validation Accuracy: 71.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.5514, Train Accuracy: 78.08%\n",
      "Epoch 4, Validation Loss: 0.8996, Validation Accuracy: 71.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.5487, Train Accuracy: 78.27%\n",
      "Epoch 5, Validation Loss: 0.9003, Validation Accuracy: 71.09%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.5525, Train Accuracy: 78.14%\n",
      "Epoch 6, Validation Loss: 0.9017, Validation Accuracy: 71.03%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.5499, Train Accuracy: 78.11%\n",
      "Epoch 7, Validation Loss: 0.8995, Validation Accuracy: 71.12%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.5519, Train Accuracy: 78.05%\n",
      "Epoch 8, Validation Loss: 0.9008, Validation Accuracy: 71.07%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.5508, Train Accuracy: 78.14%\n",
      "Epoch 9, Validation Loss: 0.9019, Validation Accuracy: 71.06%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.5512, Train Accuracy: 78.10%\n",
      "Epoch 10, Validation Loss: 0.9023, Validation Accuracy: 71.05%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.6205, Train Accuracy: 76.83%\n",
      "Epoch 1, Validation Loss: 0.7392, Validation Accuracy: 77.30%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.4968, Train Accuracy: 84.18%\n",
      "Epoch 2, Validation Loss: 0.5611, Validation Accuracy: 84.14%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.4063, Train Accuracy: 87.54%\n",
      "Epoch 3, Validation Loss: 0.5506, Validation Accuracy: 84.75%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.3544, Train Accuracy: 89.14%\n",
      "Epoch 4, Validation Loss: 0.5494, Validation Accuracy: 84.60%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.3158, Train Accuracy: 90.32%\n",
      "Epoch 5, Validation Loss: 0.5472, Validation Accuracy: 84.98%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.2933, Train Accuracy: 91.01%\n",
      "Epoch 6, Validation Loss: 0.5991, Validation Accuracy: 84.90%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.2692, Train Accuracy: 91.77%\n",
      "Epoch 7, Validation Loss: 0.6438, Validation Accuracy: 84.54%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.2483, Train Accuracy: 92.32%\n",
      "Epoch 8, Validation Loss: 0.6882, Validation Accuracy: 84.45%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.2374, Train Accuracy: 92.76%\n",
      "Epoch 9, Validation Loss: 0.6595, Validation Accuracy: 84.43%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.2223, Train Accuracy: 93.33%\n",
      "Epoch 10, Validation Loss: 0.6898, Validation Accuracy: 83.96%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.1820, Train Accuracy: 94.30%\n",
      "Epoch 1, Validation Loss: 0.7373, Validation Accuracy: 84.12%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.1768, Train Accuracy: 94.42%\n",
      "Epoch 2, Validation Loss: 0.7590, Validation Accuracy: 83.92%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.1783, Train Accuracy: 94.46%\n",
      "Epoch 3, Validation Loss: 0.7576, Validation Accuracy: 84.00%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.1746, Train Accuracy: 94.50%\n",
      "Epoch 4, Validation Loss: 0.7553, Validation Accuracy: 83.96%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.1748, Train Accuracy: 94.56%\n",
      "Epoch 5, Validation Loss: 0.7726, Validation Accuracy: 83.86%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.1753, Train Accuracy: 94.45%\n",
      "Epoch 6, Validation Loss: 0.7750, Validation Accuracy: 83.89%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.1725, Train Accuracy: 94.53%\n",
      "Epoch 7, Validation Loss: 0.7796, Validation Accuracy: 83.90%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.1719, Train Accuracy: 94.57%\n",
      "Epoch 8, Validation Loss: 0.7988, Validation Accuracy: 83.74%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.1686, Train Accuracy: 94.67%\n",
      "Epoch 9, Validation Loss: 0.7824, Validation Accuracy: 83.74%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.1696, Train Accuracy: 94.68%\n",
      "Epoch 10, Validation Loss: 0.7859, Validation Accuracy: 83.91%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.0001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.1699, Train Accuracy: 94.61%\n",
      "Epoch 1, Validation Loss: 0.8078, Validation Accuracy: 83.87%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.1647, Train Accuracy: 94.73%\n",
      "Epoch 2, Validation Loss: 0.8070, Validation Accuracy: 83.75%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.1589, Train Accuracy: 94.91%\n",
      "Epoch 3, Validation Loss: 0.8333, Validation Accuracy: 83.65%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.1554, Train Accuracy: 94.96%\n",
      "Epoch 4, Validation Loss: 0.8439, Validation Accuracy: 83.56%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.1525, Train Accuracy: 94.98%\n",
      "Epoch 5, Validation Loss: 0.8661, Validation Accuracy: 83.39%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.1480, Train Accuracy: 95.05%\n",
      "Epoch 6, Validation Loss: 0.8891, Validation Accuracy: 83.22%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.1461, Train Accuracy: 95.15%\n",
      "Epoch 7, Validation Loss: 0.9001, Validation Accuracy: 83.28%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.1465, Train Accuracy: 95.07%\n",
      "Epoch 8, Validation Loss: 0.9077, Validation Accuracy: 83.19%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.1420, Train Accuracy: 95.22%\n",
      "Epoch 9, Validation Loss: 0.9310, Validation Accuracy: 83.21%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.1405, Train Accuracy: 95.35%\n",
      "Epoch 10, Validation Loss: 0.9215, Validation Accuracy: 83.17%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.0001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.1331, Train Accuracy: 95.46%\n",
      "Epoch 1, Validation Loss: 0.9339, Validation Accuracy: 83.19%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.1343, Train Accuracy: 95.50%\n",
      "Epoch 2, Validation Loss: 0.9394, Validation Accuracy: 83.17%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.1331, Train Accuracy: 95.45%\n",
      "Epoch 3, Validation Loss: 0.9434, Validation Accuracy: 83.17%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.1317, Train Accuracy: 95.60%\n",
      "Epoch 4, Validation Loss: 0.9459, Validation Accuracy: 83.12%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.1333, Train Accuracy: 95.45%\n",
      "Epoch 5, Validation Loss: 0.9493, Validation Accuracy: 83.18%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.1325, Train Accuracy: 95.53%\n",
      "Epoch 6, Validation Loss: 0.9541, Validation Accuracy: 83.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.1336, Train Accuracy: 95.54%\n",
      "Epoch 7, Validation Loss: 0.9531, Validation Accuracy: 83.15%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.1329, Train Accuracy: 95.41%\n",
      "Epoch 8, Validation Loss: 0.9580, Validation Accuracy: 83.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.1325, Train Accuracy: 95.46%\n",
      "Epoch 9, Validation Loss: 0.9605, Validation Accuracy: 83.06%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.1334, Train Accuracy: 95.49%\n",
      "Epoch 10, Validation Loss: 0.9627, Validation Accuracy: 83.04%\n",
      "\n"
     ]
    }
   ]
  }
 ]
}