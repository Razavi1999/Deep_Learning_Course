{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use ParsBert"
   ],
   "metadata": {
    "id": "NiQIWU7a57xH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# # # Install necessary libraries\n",
    "!pip install transformers\n",
    "!pip install hazm"
   ],
   "metadata": {
    "id": "McC7UXOf56cU"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from hazm import Normalizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUXfJEK76Bo4",
    "outputId": "2ff97d68-0a8b-47b0-c554-9ae950215fc2"
   },
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive' , force_remount=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_path1 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/anger.csv'\n",
    "file_path2 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/disgust.csv'\n",
    "file_path3 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/fear.csv'\n",
    "\n",
    "file_path4 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/joy.csv'\n",
    "file_path5 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/sad.csv'\n",
    "file_path6 = '/content/drive/MyDrive/persian-tweets-emotional-dataset/surprise.csv'\n",
    "\n",
    "\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "df3 = pd.read_csv(file_path3)\n",
    "\n",
    "df4 = pd.read_csv(file_path4)\n",
    "df5 = pd.read_csv(file_path5)\n",
    "df6 = pd.read_csv(file_path6)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index = True)\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop = True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDFAic3i6Lxx",
    "outputId": "23ca396f-e562-40da-cc18-993ed40b28b0"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "stopwords = ['و', 'در', 'به' ,\n",
    "             'از' , 'که' , 'این' , 'است',\n",
    "             'این' , 'می' , 'را' , 'با' , 'های' , 'برای' , 'آن' , 'یک' ,\n",
    "             'برای' , 'های' ,\n",
    "             'خود' , 'ها' , 'کرد' , 'شد' , 'ای' , 'تا' ,\n",
    "             'کند' , 'بر' , 'بود' , 'گفت' ,\n",
    "             'نیز' , 'وی' , 'هم' , 'کنند' , 'دارد' , 'ما' , 'کرده' ,\n",
    "             'یا' , 'اما' , 'باید' ,\n",
    "             'دو' , 'اند' , 'هر' , 'خواهد' , 'او' , 'مورد' , 'باشد' ,\n",
    "             'دیگر' , 'بین' , 'پیش' ,\n",
    "             'پس' , 'اگر' , 'همه' , 'صورت' , 'یکی' , 'هستند' , 'من' , 'دهد' ,\n",
    "             'نیست' , 'استفاده' ,\n",
    "             'داد' , 'داشته' , 'راه' , 'داشت' , 'چه' , 'همچنین' ,\n",
    "             'کردند' , 'داده' , 'بوده' , 'دارند' ,\n",
    "             'همین' , 'سوی' , 'شوند' , 'بسیار' , 'روی' , 'گرفته‌اند' ,\n",
    "             'هایی' , 'تواند' , 'حتی' , 'اینکه' ,\n",
    "             'این‌که' , 'ولی' , 'توسط' , 'چنین' , 'برخی' ,\n",
    "             'درباره' , 'گیرد' , 'گفته' , 'آنان' ,\n",
    "             'بار' , 'طور' , 'گرفت' , 'دهند' , 'گذاری' , 'بسیاری' ,\n",
    "             'طی' , 'بودند' , 'براساس' , 'شدند' ,\n",
    "             'باشند' , 'چون' , 'قابل' , 'گوید' , 'دیگری' , 'همان' , 'خواهند' ,\n",
    "             'طریق' , 'آمده' , 'تحت' ,\n",
    "             'گیری' , 'جای' , 'سازی' , 'کنم' , 'زیر' , 'توانند' , 'ضمن' ,\n",
    "             'فقط' , 'بودن' , 'آید' ,\n",
    "             'اش' , 'ام' , 'ات' , 'آورد' , 'امان' , 'اتان' , 'اشان' ,\n",
    "             'آنچه' , 'ریزی' ,\n",
    "             'بنابراین' , 'بعضی' , 'برخی' , 'دادند' , 'داشتند' ,\n",
    "             'زیر' , 'روی' , 'سری' , 'توی' , 'جلوی' ,\n",
    "             'پیش' , 'عقب' , 'بالای' , 'خارج' , 'وسط' , 'بیرون' , 'سوی' , 'کنار' ,\n",
    "             'نزد' , 'دنبال' ,\n",
    "             'حدود' , 'برابر' , 'اثر' , 'علت' , 'عنوان' , 'قصد' , 'جدا' ,\n",
    "             'کی' , 'که' , 'چیست' ,\n",
    "             'هست' , 'کجا' , 'کجاست' , 'چطور' , 'کدام' , 'آیا' , 'مگر' ,\n",
    "             'چندین' , 'یک' , 'چیزی' ,\n",
    "             'دیگر' , 'کسی' , 'چیز' , 'جا' , 'کس' , 'لطفا' , 'تان' , 'مان' ,\n",
    "             'هنگامی' , 'وقتیکه' ,\n",
    "             'مدتی' ,\n",
    "             'آنکه' , 'انکه' , 'خواهشا' , 'وقتیکه'\n",
    "             ]\n",
    "\n",
    "# print('----------- length of stopwords ---------------')\n",
    "# print(len(stopwords))\n",
    "# print('-----------------------------------------------')\n",
    "# print('مدتی' in stopwords)"
   ],
   "metadata": {
    "id": "DdFeDx016Olk"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "emoji_mapping = {\n",
    "    '😊': 'خندان',\n",
    "    '😢': 'غمگین',\n",
    "    '😂': 'خنده',\n",
    "    '😍': 'عاشق',\n",
    "    '❤️': 'عشق',\n",
    "    '👍': 'خوب',\n",
    "    '👎': 'بد',\n",
    "    '🙏': 'دعا',\n",
    "    '🔥': 'آتش',\n",
    "    '🔴' : 'قرمز' ,\n",
    "    '🟡' : 'زرد' ,\n",
    "    '✊' : 'مشت' ,\n",
    "    '💥' : 'هیجان' ,\n",
    "    '📡' : 'فوری' ,\n",
    "    '⬅️' : 'تماشا' ,\n",
    "    '🇮🇷' : 'ایران' ,\n",
    "    '✌' : 'دوباره' ,\n",
    "    '💔': 'شکسته',\n",
    "    '🌸': 'گل',\n",
    "    '🌞': 'خورشید',\n",
    "    '🌈': 'رنگین‌کمان',\n",
    "    '🎉': 'تبریک',\n",
    "    '🤗': 'دوستانه',\n",
    "    '🤔': 'تفکر',\n",
    "    '😜': 'شوخی'\n",
    "}"
   ],
   "metadata": {
    "id": "b1bDvXTt6S5r"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing tweets"
   ],
   "metadata": {
    "id": "Thh0Ic1h6XLd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from hazm import Normalizer, Stemmer, word_tokenize\n",
    "\n",
    "\n",
    "normalizer = Normalizer()\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet , stop_words , emoji_mapping):\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|<\\S+>', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1', tweet)\n",
    "\n",
    "    tweet = normalizer.normalize(tweet)\n",
    "    words = word_tokenize(tweet)\n",
    "\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "\n",
    "\n",
    "    for emoji, text in emoji_mapping.items():\n",
    "        tweet = tweet.replace(emoji, text)\n",
    "\n",
    "    stemmed_words = [word for word in words]\n",
    "\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "df['preprocessed_tweet'] = df['tweet'].apply(lambda x: preprocess_tweet(x, stopwords, emoji_mapping))\n",
    "\n",
    "# df.sample(7)[['hashtags' , 'tweet', 'emotion', 'preprocessed_tweet']]"
   ],
   "metadata": {
    "id": "cHvLiudq6eFS"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenizing"
   ],
   "metadata": {
    "id": "eZHEQ9ob6kmg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "def tokenize_and_pad(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, max_length=32, return_tensors=\"pt\")\n",
    "    return tokens[\"input_ids\"]\n",
    "\n",
    "\n",
    "df['ids'] = df['preprocessed_tweet'].apply(tokenize_and_pad)\n",
    "# df.sample(7)[['hashtags' , 'tweet', 'emotion', 'preprocessed_tweet' , 'ids']]"
   ],
   "metadata": {
    "id": "tYfmHPYz6m_w",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9992293-f471-4839-8b06-cb59895c7faf"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Dataset"
   ],
   "metadata": {
    "id": "HbSzYojj6rzL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "val_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "id": "s8QvjYT86uWY"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "emotion_to_label = {\n",
    "    'anger': 0,\n",
    "    'disgust': 1,\n",
    "    'fear': 2,\n",
    "    'joy': 3,\n",
    "    'sad': 4,\n",
    "    'surprise': 5\n",
    "}\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['preprocessed_tweet']\n",
    "        emotion = self.data.iloc[idx]['emotion']\n",
    "\n",
    "        label = emotion_to_label[emotion]\n",
    "        tokens = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "\n",
    "        input_ids = tokens\n",
    "        # padding_length = self.max_length - input_ids.shape[1]\n",
    "        # if padding_length > 0:\n",
    "        #     input_ids = torch.cat([input_ids, torch.zeros((1, padding_length), dtype=torch.long)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'text' : text ,\n",
    "            'input_ids': input_ids,\n",
    "            'label': torch.tensor(label)\n",
    "        }\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-base-parsbert-uncased\")\n",
    "\n",
    "train_dataset = CustomDataset(train_data, tokenizer , max_length=32)\n",
    "val_dataset   = CustomDataset(val_data  , tokenizer , max_length=32)\n",
    "test_dataset  = CustomDataset(test_data,  tokenizer , max_length=32)"
   ],
   "metadata": {
    "id": "uO33oh5L6y0x",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b754cf6d-f5c7-4a95-cc6d-cace47bed60d"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LSTM Model"
   ],
   "metadata": {
    "id": "XbxLnMNr6aUz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ids = train_data['preprocessed_tweet'].apply(tokenize_and_pad)\n",
    "\n",
    "vocab_size = len(ids)\n",
    "\n",
    "print(f'vocab_size  : {vocab_size}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znmIwyma_Yjg",
    "outputId": "c9527595-22ed-48a7-9359-30ad19ffe7c9"
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab_size  : 79680\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_MuSga4dTGQl"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers , bidirectional, dropout):\n",
    "#         super(LSTM, self).__init__()\n",
    "\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "#     def forward(self, text):\n",
    "#         text = torch.where(text < self.vocab_size , text , torch.tensor( self.vocab_size - 1 ))\n",
    "#         embedded = self.embedding(text)\n",
    "#         # text shape: [seq_len, batch_size]\n",
    "\n",
    "#         output, (hidden, cell) = self.lstm(embedded)  # output shape: [seq_len, batch_size, hidden_dim * num_directions]\n",
    "\n",
    "#         hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))  # Concatenating the final hidden state from both directions\n",
    "\n",
    "#         output = self.fc(hidden)\n",
    "\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=1, dropout=0.5, pad_idx=0):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # Initialize the LSTM layer with bidirectional set to True\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                            bidirectional=True, batch_first=True)\n",
    "        # Since the LSTM is bidirectional, the output dimensions will be doubled\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        clamped_text = torch.clamp(text, 0, self.vocab_size - 1)\n",
    "\n",
    "        # text is [batch size, sent len]\n",
    "        embedded = self.embedding(clamped_text)\n",
    "        # embedded is [batch size, sent len, emb dim]\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        dropped = self.dropout(hidden)\n",
    "\n",
    "\n",
    "        output = self.fc(dropped)\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "id": "6bOF9JYrGRD8"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Finding Optimal Hyperparameters"
   ],
   "metadata": {
    "id": "E_EigGg-_DQs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'vocab_size  : {vocab_size}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xgMJZbENDTeb",
    "outputId": "860e39e3-a7a9-44f6-ef45-5fd2eac4ad74"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocab_size  : 79680\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# hyperparameters\n",
    "batch_sizes = [64, 8]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "optimizers = {'Adam': Adam, 'SGD': SGD}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for batch in train_loader:\n",
    "            data = batch['input_ids']['input_ids'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "\n",
    "            data = data.squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print('-----------------------------------------------')\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                data = batch['input_ids']['input_ids'].to(device)\n",
    "                targets = batch['label'].to(device)\n",
    "                data = data.squeeze(1)\n",
    "                outputs = model(data)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "        print()\n",
    "\n",
    "\n",
    "model = LSTM_Model(vocab_size, embedding_dim = 120 , hidden_dim = 6 , output_dim = 6 , num_layers = 2 ,  dropout = 0.5 )\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for opt_name, Optimizer in optimizers.items():\n",
    "            print(f'Testing batch size {batch_size}, learning ratne {lr}, optimizer {opt_name}')\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            optimizer = Optimizer(model.parameters(), lr=lr)\n",
    "            train_model(model, criterion, optimizer, train_loader, val_loader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QK0ndrpz-aJo",
    "outputId": "6c8ca132-14fc-4262-ddde-4123678f0ebd"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing batch size 64, learning ratne 0.001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 1.4965, Train Accuracy: 35.15%\n",
      "Epoch 1, Validation Loss: 1.2640, Validation Accuracy: 48.33%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 1.1773, Train Accuracy: 52.39%\n",
      "Epoch 2, Validation Loss: 1.0773, Validation Accuracy: 56.62%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 1.0231, Train Accuracy: 60.49%\n",
      "Epoch 3, Validation Loss: 0.9830, Validation Accuracy: 63.53%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.9188, Train Accuracy: 65.66%\n",
      "Epoch 4, Validation Loss: 0.9472, Validation Accuracy: 65.65%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.8614, Train Accuracy: 67.60%\n",
      "Epoch 5, Validation Loss: 0.9352, Validation Accuracy: 67.70%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.8171, Train Accuracy: 69.68%\n",
      "Epoch 6, Validation Loss: 0.9251, Validation Accuracy: 67.97%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.7822, Train Accuracy: 70.69%\n",
      "Epoch 7, Validation Loss: 0.9337, Validation Accuracy: 67.85%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.7440, Train Accuracy: 72.18%\n",
      "Epoch 8, Validation Loss: 0.9197, Validation Accuracy: 68.95%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.7205, Train Accuracy: 72.66%\n",
      "Epoch 9, Validation Loss: 0.9250, Validation Accuracy: 68.60%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.6800, Train Accuracy: 74.32%\n",
      "Epoch 10, Validation Loss: 0.8824, Validation Accuracy: 70.30%\n",
      "\n",
      "Testing batch size 64, learning ratne 0.001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.6381, Train Accuracy: 75.75%\n",
      "Epoch 1, Validation Loss: 0.8977, Validation Accuracy: 69.79%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.6353, Train Accuracy: 75.82%\n",
      "Epoch 2, Validation Loss: 0.8872, Validation Accuracy: 70.27%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.6346, Train Accuracy: 75.81%\n",
      "Epoch 3, Validation Loss: 0.8912, Validation Accuracy: 70.25%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.6329, Train Accuracy: 75.79%\n",
      "Epoch 4, Validation Loss: 0.8861, Validation Accuracy: 70.33%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.6330, Train Accuracy: 75.86%\n",
      "Epoch 5, Validation Loss: 0.8891, Validation Accuracy: 70.42%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.6327, Train Accuracy: 76.05%\n",
      "Epoch 6, Validation Loss: 0.8878, Validation Accuracy: 70.27%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.6326, Train Accuracy: 75.92%\n",
      "Epoch 7, Validation Loss: 0.8908, Validation Accuracy: 70.28%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.6284, Train Accuracy: 75.88%\n",
      "Epoch 8, Validation Loss: 0.8869, Validation Accuracy: 70.32%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.6261, Train Accuracy: 75.98%\n",
      "Epoch 9, Validation Loss: 0.8906, Validation Accuracy: 70.42%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.6254, Train Accuracy: 75.93%\n",
      "Epoch 10, Validation Loss: 0.8907, Validation Accuracy: 70.38%\n",
      "\n",
      "Testing batch size 64, learning ratne 0.0001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.6244, Train Accuracy: 76.15%\n",
      "Epoch 1, Validation Loss: 0.8919, Validation Accuracy: 70.57%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.6157, Train Accuracy: 76.42%\n",
      "Epoch 2, Validation Loss: 0.8882, Validation Accuracy: 70.58%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.6095, Train Accuracy: 76.49%\n",
      "Epoch 3, Validation Loss: 0.8956, Validation Accuracy: 70.61%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.6008, Train Accuracy: 76.80%\n",
      "Epoch 4, Validation Loss: 0.8904, Validation Accuracy: 70.48%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.5955, Train Accuracy: 76.87%\n",
      "Epoch 5, Validation Loss: 0.8984, Validation Accuracy: 70.56%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.5889, Train Accuracy: 77.03%\n",
      "Epoch 6, Validation Loss: 0.9005, Validation Accuracy: 70.68%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.5821, Train Accuracy: 77.26%\n",
      "Epoch 7, Validation Loss: 0.8997, Validation Accuracy: 70.78%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.5720, Train Accuracy: 77.52%\n",
      "Epoch 8, Validation Loss: 0.8963, Validation Accuracy: 70.83%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.5629, Train Accuracy: 77.77%\n",
      "Epoch 9, Validation Loss: 0.8985, Validation Accuracy: 70.94%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.5609, Train Accuracy: 77.81%\n",
      "Epoch 10, Validation Loss: 0.8992, Validation Accuracy: 70.95%\n",
      "\n",
      "Testing batch size 64, learning ratne 0.0001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.5495, Train Accuracy: 78.18%\n",
      "Epoch 1, Validation Loss: 0.8997, Validation Accuracy: 71.03%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.5513, Train Accuracy: 78.10%\n",
      "Epoch 2, Validation Loss: 0.8997, Validation Accuracy: 71.03%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.5504, Train Accuracy: 78.08%\n",
      "Epoch 3, Validation Loss: 0.9001, Validation Accuracy: 71.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.5514, Train Accuracy: 78.08%\n",
      "Epoch 4, Validation Loss: 0.8996, Validation Accuracy: 71.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.5487, Train Accuracy: 78.27%\n",
      "Epoch 5, Validation Loss: 0.9003, Validation Accuracy: 71.09%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.5525, Train Accuracy: 78.14%\n",
      "Epoch 6, Validation Loss: 0.9017, Validation Accuracy: 71.03%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.5499, Train Accuracy: 78.11%\n",
      "Epoch 7, Validation Loss: 0.8995, Validation Accuracy: 71.12%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.5519, Train Accuracy: 78.05%\n",
      "Epoch 8, Validation Loss: 0.9008, Validation Accuracy: 71.07%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.5508, Train Accuracy: 78.14%\n",
      "Epoch 9, Validation Loss: 0.9019, Validation Accuracy: 71.06%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.5512, Train Accuracy: 78.10%\n",
      "Epoch 10, Validation Loss: 0.9023, Validation Accuracy: 71.05%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.6205, Train Accuracy: 76.83%\n",
      "Epoch 1, Validation Loss: 0.7392, Validation Accuracy: 77.30%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.4968, Train Accuracy: 84.18%\n",
      "Epoch 2, Validation Loss: 0.5611, Validation Accuracy: 84.14%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.4063, Train Accuracy: 87.54%\n",
      "Epoch 3, Validation Loss: 0.5506, Validation Accuracy: 84.75%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.3544, Train Accuracy: 89.14%\n",
      "Epoch 4, Validation Loss: 0.5494, Validation Accuracy: 84.60%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.3158, Train Accuracy: 90.32%\n",
      "Epoch 5, Validation Loss: 0.5472, Validation Accuracy: 84.98%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.2933, Train Accuracy: 91.01%\n",
      "Epoch 6, Validation Loss: 0.5991, Validation Accuracy: 84.90%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.2692, Train Accuracy: 91.77%\n",
      "Epoch 7, Validation Loss: 0.6438, Validation Accuracy: 84.54%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.2483, Train Accuracy: 92.32%\n",
      "Epoch 8, Validation Loss: 0.6882, Validation Accuracy: 84.45%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.2374, Train Accuracy: 92.76%\n",
      "Epoch 9, Validation Loss: 0.6595, Validation Accuracy: 84.43%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.2223, Train Accuracy: 93.33%\n",
      "Epoch 10, Validation Loss: 0.6898, Validation Accuracy: 83.96%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.1820, Train Accuracy: 94.30%\n",
      "Epoch 1, Validation Loss: 0.7373, Validation Accuracy: 84.12%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.1768, Train Accuracy: 94.42%\n",
      "Epoch 2, Validation Loss: 0.7590, Validation Accuracy: 83.92%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.1783, Train Accuracy: 94.46%\n",
      "Epoch 3, Validation Loss: 0.7576, Validation Accuracy: 84.00%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.1746, Train Accuracy: 94.50%\n",
      "Epoch 4, Validation Loss: 0.7553, Validation Accuracy: 83.96%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.1748, Train Accuracy: 94.56%\n",
      "Epoch 5, Validation Loss: 0.7726, Validation Accuracy: 83.86%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.1753, Train Accuracy: 94.45%\n",
      "Epoch 6, Validation Loss: 0.7750, Validation Accuracy: 83.89%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.1725, Train Accuracy: 94.53%\n",
      "Epoch 7, Validation Loss: 0.7796, Validation Accuracy: 83.90%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.1719, Train Accuracy: 94.57%\n",
      "Epoch 8, Validation Loss: 0.7988, Validation Accuracy: 83.74%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.1686, Train Accuracy: 94.67%\n",
      "Epoch 9, Validation Loss: 0.7824, Validation Accuracy: 83.74%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.1696, Train Accuracy: 94.68%\n",
      "Epoch 10, Validation Loss: 0.7859, Validation Accuracy: 83.91%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.0001, optimizer Adam\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.1699, Train Accuracy: 94.61%\n",
      "Epoch 1, Validation Loss: 0.8078, Validation Accuracy: 83.87%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.1647, Train Accuracy: 94.73%\n",
      "Epoch 2, Validation Loss: 0.8070, Validation Accuracy: 83.75%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.1589, Train Accuracy: 94.91%\n",
      "Epoch 3, Validation Loss: 0.8333, Validation Accuracy: 83.65%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.1554, Train Accuracy: 94.96%\n",
      "Epoch 4, Validation Loss: 0.8439, Validation Accuracy: 83.56%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.1525, Train Accuracy: 94.98%\n",
      "Epoch 5, Validation Loss: 0.8661, Validation Accuracy: 83.39%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.1480, Train Accuracy: 95.05%\n",
      "Epoch 6, Validation Loss: 0.8891, Validation Accuracy: 83.22%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.1461, Train Accuracy: 95.15%\n",
      "Epoch 7, Validation Loss: 0.9001, Validation Accuracy: 83.28%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.1465, Train Accuracy: 95.07%\n",
      "Epoch 8, Validation Loss: 0.9077, Validation Accuracy: 83.19%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.1420, Train Accuracy: 95.22%\n",
      "Epoch 9, Validation Loss: 0.9310, Validation Accuracy: 83.21%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.1405, Train Accuracy: 95.35%\n",
      "Epoch 10, Validation Loss: 0.9215, Validation Accuracy: 83.17%\n",
      "\n",
      "Testing batch size 8, learning ratne 0.0001, optimizer SGD\n",
      "-----------------------------------------------\n",
      "Epoch 1, Train Loss: 0.1331, Train Accuracy: 95.46%\n",
      "Epoch 1, Validation Loss: 0.9339, Validation Accuracy: 83.19%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 2, Train Loss: 0.1343, Train Accuracy: 95.50%\n",
      "Epoch 2, Validation Loss: 0.9394, Validation Accuracy: 83.17%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 3, Train Loss: 0.1331, Train Accuracy: 95.45%\n",
      "Epoch 3, Validation Loss: 0.9434, Validation Accuracy: 83.17%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 4, Train Loss: 0.1317, Train Accuracy: 95.60%\n",
      "Epoch 4, Validation Loss: 0.9459, Validation Accuracy: 83.12%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 5, Train Loss: 0.1333, Train Accuracy: 95.45%\n",
      "Epoch 5, Validation Loss: 0.9493, Validation Accuracy: 83.18%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 6, Train Loss: 0.1325, Train Accuracy: 95.53%\n",
      "Epoch 6, Validation Loss: 0.9541, Validation Accuracy: 83.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 7, Train Loss: 0.1336, Train Accuracy: 95.54%\n",
      "Epoch 7, Validation Loss: 0.9531, Validation Accuracy: 83.15%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 8, Train Loss: 0.1329, Train Accuracy: 95.41%\n",
      "Epoch 8, Validation Loss: 0.9580, Validation Accuracy: 83.08%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 9, Train Loss: 0.1325, Train Accuracy: 95.46%\n",
      "Epoch 9, Validation Loss: 0.9605, Validation Accuracy: 83.06%\n",
      "\n",
      "-----------------------------------------------\n",
      "Epoch 10, Train Loss: 0.1334, Train Accuracy: 95.49%\n",
      "Epoch 10, Validation Loss: 0.9627, Validation Accuracy: 83.04%\n",
      "\n"
     ]
    }
   ]
  }
 ]
}