{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT_xIZiQUZe7"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPlwBepoRAHD",
        "outputId": "1e677cee-3c9c-41c4-c11e-80fbbe5f6005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X4E5fcv7Scoq"
      },
      "outputs": [],
      "source": [
        "test_path = '/content/drive/MyDrive/NNDL_HW5/Test.csv'\n",
        "train_path = '/content/drive/MyDrive/NNDL_HW5/Train.csv'\n",
        "val_path = '/content/drive/MyDrive/NNDL_HW5/Val.csv'\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "test_df = pd.read_csv(test_path)\n",
        "train_df = pd.read_csv(train_path)\n",
        "val_df = pd.read_csv(val_path)\n",
        "\n",
        "\n",
        "# train_df.head()\n",
        "# val_df.head()\n",
        "# test_df.sample(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "I3KpJMbnC4GE",
        "outputId": "2d7221a8-a574-4355-c355-b7b135b0b274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  label\n",
              "0        1  The CDC currently reports 99031 deaths. In gen...      1\n",
              "1        2  States reported 1121 deaths a small rise from ...      1\n",
              "2        3  Politically Correct Woman (Almost) Uses Pandem...      0\n",
              "3        4  #IndiaFightsCorona: We have 1524 #COVID testin...      1\n",
              "4        5  Populous states can generate large case counts...      1\n",
              "...    ...                                                ...    ...\n",
              "6415  6416  A tiger tested positive for COVID-19 please st...      0\n",
              "6416  6417  ???Autopsies prove that COVID-19 is??� a blood...      0\n",
              "6417  6418  _A post claims a COVID-19 vaccine has already ...      0\n",
              "6418  6419  Aamir Khan Donate 250 Cr. In PM Relief Cares Fund      0\n",
              "6419  6420  It has been 93 days since the last case of COV...      1\n",
              "\n",
              "[6420 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-678d9961-aa64-4963-a67f-ca126548c072\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The CDC currently reports 99031 deaths. In gen...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>States reported 1121 deaths a small rise from ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Politically Correct Woman (Almost) Uses Pandem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>#IndiaFightsCorona: We have 1524 #COVID testin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Populous states can generate large case counts...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6415</th>\n",
              "      <td>6416</td>\n",
              "      <td>A tiger tested positive for COVID-19 please st...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6416</th>\n",
              "      <td>6417</td>\n",
              "      <td>???Autopsies prove that COVID-19 is??� a blood...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6417</th>\n",
              "      <td>6418</td>\n",
              "      <td>_A post claims a COVID-19 vaccine has already ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6418</th>\n",
              "      <td>6419</td>\n",
              "      <td>Aamir Khan Donate 250 Cr. In PM Relief Cares Fund</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6419</th>\n",
              "      <td>6420</td>\n",
              "      <td>It has been 93 days since the last case of COV...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6420 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-678d9961-aa64-4963-a67f-ca126548c072')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-678d9961-aa64-4963-a67f-ca126548c072 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-678d9961-aa64-4963-a67f-ca126548c072');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8ac7c5a3-f5ed-4cf5-bd30-372a8013b907\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ac7c5a3-f5ed-4cf5-bd30-372a8013b907')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8ac7c5a3-f5ed-4cf5-bd30-372a8013b907 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 6420,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1853,\n        \"min\": 1,\n        \"max\": 6420,\n        \"num_unique_values\": 6420,\n        \"samples\": [\n          325,\n          1341,\n          6026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6420,\n        \"samples\": [\n          \"Canada\\u2019s top BDSM doctor says wear a mask, leash, tight leather to prevent spread of COVID-19 #cdnpoli #COVID19 https://t.co/1E7yDlIGBD https://t.co/bffEps28Iy\",\n          \"There are 3 cases considered to have recovered from COVID-19 so our total number of active cases is 23 \\u2013 all remain in quarantine facilities.\\u200b \\u200b Our total number of confirmed cases remains at 1192 which is the number we report to the World Health Organization.\",\n          \"Heard about contact tracing but not sure what it is? It\\u2019s used by health departments to prevent the spread of #COVID19. Learn more: https://t.co/J3Txu3riWr. #SlowtheSpread https://t.co/3f8aEQCTuI\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_df['label'] = train_df['label'].map({\"real\" : 1 , \"fake\" : 0})\n",
        "val_df['label'] = val_df['label'].map({\"real\" : 1 , \"fake\" : 0})\n",
        "\n",
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDM3XqLuUhgd"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I37KNFGjCoBl"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GYrqU5AiUlZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "9fef8f74-53e0-4181-efd2-c68ba86a9e9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a0ba36f7988d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as functional\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "import gc\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import roc_auc_score,f1_score\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV6ogOScDrc4"
      },
      "outputs": [],
      "source": [
        "data = pd.concat([train_df , val_df], axis=0, ignore_index=True).drop([\"id\"], axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXAL3eGiEZZP"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                          do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOqP_qkDEvkd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "\n",
        "def preprocess(data):\n",
        "    for i in range(data.shape[0]):\n",
        "        text = data[i].lower()\n",
        "        text1 = ''.join([ word + \" \" for word in text.split()])\n",
        "        data[i] = text1\n",
        "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*,]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    mention_regex = '@[\\w\\-]+'\n",
        "    hashtag_regex = '#[\\w\\-]+'\n",
        "    space_pattern = '\\s+'\n",
        "\n",
        "    samples_count = data.shape[0]\n",
        "\n",
        "    for i in range(samples_count):\n",
        "        text_string = data[i]\n",
        "        parsed_text = re.sub(hashtag_regex, '', text_string)\n",
        "        parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "        parsed_text = re.sub(mention_regex, '', parsed_text)\n",
        "\n",
        "        parsed_text = re.sub(r\"[{}]+\".format(punctuation), '', parsed_text)\n",
        "        parsed_text = re.sub(space_pattern, ' ', parsed_text)\n",
        "        data[i] = parsed_text\n",
        "\n",
        "    return data\n",
        "\n",
        "tweets = data.tweet.values\n",
        "tweets = preprocess(tweets)\n",
        "print(tweets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWjgX5h7HZ68"
      },
      "outputs": [],
      "source": [
        "print(' Original: \\n', tweets[10])\n",
        "\n",
        "print('Tokenized: \\n ', tokenizer.tokenize(tweets[10]))\n",
        "\n",
        "\n",
        "print('Token IDs: \\n',\n",
        "      tokenizer.convert_tokens_to_ids(tokenizer.tokenize(tweets[10])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V87utMlIIgR"
      },
      "outputs": [],
      "source": [
        "tweets = data.tweet.values\n",
        "labels = data.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJcuR-jCH8Ue"
      },
      "outputs": [],
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# Add the encoded sentence And its attention mask to the list.\n",
        "\n",
        "for tweet in tweets:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        tweet,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 128,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', tweets[10])\n",
        "print('Token IDs:', input_ids[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCV6BHd1IpgQ"
      },
      "source": [
        "# Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zKE8zhrIrcP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], \\\n",
        "                            generator = torch.Generator().manual_seed(42))\n",
        "\n",
        "print('{} training   samples'.format(train_size))\n",
        "print('{} validation samples'.format(val_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ2XdUyCJHUk"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler,\\\n",
        "SequentialSampler\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            shuffle = True,\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            shuffle = False,\n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5qpf3pvJLAN"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MV3O5uPfs8z"
      },
      "source": [
        "# (1) BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooUxGaw3JR6D"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     \"bert-base-uncased\",\n",
        "#     num_labels = 2,\n",
        "#     output_attentions = False,\n",
        "#     output_hidden_states = False,\n",
        "# )\n",
        "\n",
        "from transformers import BertModel\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float32)\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYF01rDtJ6iX"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 10e-5,\n",
        "                  eps = 1e-8\n",
        "                )\n",
        "epochs = 3\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilj2NVmVNGYq"
      },
      "source": [
        "$ JavaScript $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPPbFZxFC_9Z"
      },
      "outputs": [],
      "source": [
        "# function ConnectButton(){\n",
        "#   console.log(\"Connect pushed\");\n",
        "#   document.querySelector(\"#top-toolbar > colab-connectbutton\").shadowRoot.querySelector(\"#connect\").click()\n",
        "# }\n",
        "# setInterval(ConnectButton , 40000);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGUvw1GhNDtH"
      },
      "source": [
        "# Bert Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, model_tune):\n",
        "        super().__init__()\n",
        "        self.bert = model_tune\n",
        "        self.classifier = nn.Linear(768 , 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        input_ids = input_ids.to(torch.long)\n",
        "        attention_mask = attention_mask.to(torch.float32)\n",
        "\n",
        "        if token_type_ids is not None:\n",
        "            token_type_ids = token_type_ids.to(torch.float32)\n",
        "\n",
        "        bert_output = self.bert(input_ids = input_ids,\n",
        "                                attention_mask = attention_mask,\n",
        "                                token_type_ids = token_type_ids)\n",
        "\n",
        "        pooler = bert_output.pooler_output\n",
        "        logits = self.classifier(pooler)\n",
        "        pred = self.sigmoid(logits)\n",
        "        pred = (pred >= 0.5)\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "qtHSicOGp91z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = BertClassifier(model)\n",
        "model1 = model1.to(torch.float32)\n",
        "\n",
        "model1.to(device)\n",
        "\n",
        "for param in model1.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "mT56ozZ0f8Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzuKj8qvKg-J"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('Epoch {}'.format(epoch_i + 1))\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model1.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model1.zero_grad()\n",
        "        out = model1(input_ids , attention_mask=input_mask , token_type_ids=None)\n",
        "        loss = out[0]\n",
        "        logits = out[1]\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        # print(loss.requires_grad)\n",
        "        for param in model1.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        pred = (pred >= 0.5).long()\n",
        "\n",
        "        total_train_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_accuracy)\n",
        "\n",
        "    print(\"Training Accuracy : {}\".format(avg_train_accuracy))\n",
        "    print(\"Training loss : {}\".format(avg_train_loss))\n",
        "    print('------------------------------------------')\n",
        "\n",
        "    model1.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model1(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
        "            loss = out[0]\n",
        "            logits = out[1]\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # pred = (pred >= 0.5).long()\n",
        "        # print(f'pred : {pred}')\n",
        "\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(avg_val_accuracy)\n",
        "\n",
        "    print(\"Validation Accuracy: {}\".format(avg_val_accuracy))\n",
        "    print(\"Validation loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print('---------------------------------------------------\\n\\n')\n",
        "\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print(\"This epoch took: {:}\".format(training_time))\n",
        "    print('roc_auc score: ', roc_auc_score(y_true, y_pred))\n",
        "    print('F1 score : ', f1_score(y_true, y_pred))\n",
        "    print()\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid_Loss': avg_val_loss,\n",
        "            'Valid_Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_range = range(1, epochs + 1)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AOHDBBo5kM21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "0sn5YquElSJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have defined BertClassifier somewhere earlier\n",
        "model1 = BertClassifier(model)\n",
        "model1 = model1.to(device)\n",
        "\n",
        "# Set all the parameters to require gradients\n",
        "for param in model1.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "loss_function = nn.BCELoss()  # Assuming binary classification\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "for epoch_i in range(epochs):\n",
        "    print('Epoch {}'.format(epoch_i + 1))\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model1.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[0].to(device).to(torch.long)  # Ensure input IDs are Long\n",
        "        input_mask = batch[1].to(device).to(torch.float32)  # Ensure mask is Float\n",
        "        labels = batch[2].to(device).to(torch.float32)  # Ensure labels are Float\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model1(input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_function(outputs, labels)  # BCELoss expects probabilities\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()  # Compute gradients\n",
        "\n",
        "        # Clip the gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate predictions\n",
        "        preds = (outputs >= 0.5).long()  # Binary prediction using 0.5 threshold\n",
        "\n",
        "        # Calculate the accuracy\n",
        "        total_train_accuracy += torch.sum(preds == labels.long()).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_accuracy)\n",
        "\n",
        "    print(\"Training Accuracy: {}\".format(avg_train_accuracy))\n",
        "    print(\"Training Loss: {}\".format(avg_train_loss))\n",
        "    print('------------------------------------------')\n",
        "\n",
        "    model1.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device).to(torch.long)  # Ensure input IDs are Long\n",
        "        input_mask = batch[1].to(device).to(torch.float32)  # Ensure mask is Float\n",
        "        labels = batch[2].to(device).to(torch.float32)  # Ensure labels are Float\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation for evaluation\n",
        "            outputs = model1(input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_function(outputs, labels)  # BCELoss expects probabilities\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Calculate predictions\n",
        "            preds = (outputs >= 0.5).long()  # Binary prediction using 0.5 threshold\n",
        "\n",
        "            total_eval_accuracy += torch.sum(preds == labels.long()).item()\n",
        "            y_true.append(labels.flatten())\n",
        "            y_pred.append(preds.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(avg_val_accuracy)\n",
        "\n",
        "    print(\"Validation Accuracy: {}\".format(avg_val_accuracy))\n",
        "    print(\"Validation Loss: {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print('---------------------------------------------------\\n\\n')\n",
        "\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print(\"This epoch took: {:}\".format(training_time))\n",
        "    print('roc_auc score: ', roc_auc_score(y_true, y_pred))\n",
        "    print('F1 score: ', f1_score(y_true, y_pred))\n",
        "    print()\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid_Loss': avg_val_loss,\n",
        "            'Valid_Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model1\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "\n",
        "# Plotting training and validation loss/accuracy\n",
        "epochs_range = range(1, epochs + 1)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uxvUPFhu-yqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 2"
      ],
      "metadata": {
        "id": "UkIp7RqelULz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the training data\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "    # Transfer the batch tensors to the device (GPU or CPU)\n",
        "    input_ids = batch[0].to(device)\n",
        "    input_mask = batch[1].to(device)\n",
        "    labels = batch[2].to(device)\n",
        "\n",
        "    # Ensure the input tensors are of integer types for models like BERT\n",
        "    # (This depends on the model you're using; BERT usually takes long tensors)\n",
        "    print(type(input_ids), type(input_mask), type(labels))\n",
        "\n",
        "    # Usually, we don't need to set input requires_grad for inputs,\n",
        "    # only for model parameters and outputs.\n",
        "\n",
        "    # Forward pass: Get the model's predictions\n",
        "    outputs = model1(input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "\n",
        "    # The outputs typically contain loss in the first element (out[0])\n",
        "    # and logits in the second element (out[1])\n",
        "    loss = outputs[0]  # Assuming outputs[0] is the loss\n",
        "\n",
        "    # Accumulate the loss for logging or other purposes\n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    # Zero the gradients of all model parameters\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform backpropagation: Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the gradient norm to avoid exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(model1.parameters(), 1.0)\n",
        "\n",
        "    # Update the model parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # If needed, print the requires_grad status of model parameters\n",
        "    # for param in model1.parameters():\n",
        "    #     print(param.requires_grad)\n"
      ],
      "metadata": {
        "id": "MCQqBrszlVm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ph9uWFEOCI"
      },
      "source": [
        "# (2) Bert With Freezing Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDuTiGXvERwK"
      },
      "outputs": [],
      "source": [
        "model2 = best_model.cuda()\n",
        "for param in model2.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "epochs = 3\n",
        "learning_rate = 10e-5\n",
        "optimizer = AdamW(model.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UUvP_LCGHL9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('Epoch {}'.format(epoch_i + 1))\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model2.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model2.zero_grad()\n",
        "        out = model2(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n",
        "        loss = out[0]\n",
        "        logits = out[1]\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        total_train_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_accuracy)\n",
        "\n",
        "    print(\"Training Accuracy : {}\".format(avg_train_accuracy))\n",
        "    print(\"Training loss : {}\".format(avg_train_loss))\n",
        "    print('------------------------------------------')\n",
        "\n",
        "    model2.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model2(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n",
        "            loss = out[0]\n",
        "            logits = out[1]\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(avg_val_accuracy)\n",
        "\n",
        "    print(\"Validation Accuracy : {}\".format(avg_val_accuracy))\n",
        "    print(\"Validation Loss : {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print('---------------------------------------------------\\n\\n')\n",
        "\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print(\"This epoch took: {:}\".format(training_time))\n",
        "    print('roc_auc score: ', roc_auc_score(y_true, y_pred))\n",
        "    print('F1 score : ', f1_score(y_true, y_pred))\n",
        "    print()\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid_Loss': avg_val_loss,\n",
        "            'Valid_Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "    print()\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model\n",
        "\n",
        "print()\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B19BhIGBNO9S"
      },
      "source": [
        "# (3) Bert BiGRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "165FIb4jNRwl"
      },
      "outputs": [],
      "source": [
        "class BertGRUClassifier(nn.Module):\n",
        "    def __init__(self, model_tune):\n",
        "        super().__init__()\n",
        "        self.bert = model_tune\n",
        "        self.gru = nn.GRU(  input_size = 768,\n",
        "                            hidden_size = 128,\n",
        "                            num_layers = 1,\n",
        "                            batch_first = True,\n",
        "                            bidirectional = True)\n",
        "        self.classifier = nn.Linear(128 * 2, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        bert_output = self.bert(input_ids = input_ids,\n",
        "                                attention_mask = attention_mask,\n",
        "                                token_type_ids = token_type_ids)\n",
        "\n",
        "        out , hidden = self.gru(bert_output)\n",
        "        concatenated = torch.cat(hidden[0 , : , :] , hidden[1 , : , :] , dim = 1)\n",
        "\n",
        "        logits = self.classifier(concatenated)\n",
        "        return self.sigmoid(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0kadecEXCoZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import AdamW\n",
        "from torch import nn\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "\n",
        "model4 = BertGRUClassifier(model).cuda()\n",
        "epochs = 3\n",
        "learning_rate = 10e-5\n",
        "optimizer = AdamW(model4.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('Epoch {}'.format(epoch_i + 1))\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model4.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model4.zero_grad()\n",
        "        out = model4(input_ids=input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model4.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        total_train_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_accuracy)\n",
        "\n",
        "    print(\"Training Accuracy : {}\".format(avg_train_accuracy))\n",
        "    print(\"Training Loss : {}\".format(avg_train_loss))\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "    model4.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model4(input_ids=input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(avg_val_accuracy)\n",
        "\n",
        "    print(\"Validation Accuracy : {}\".format(avg_val_accuracy))\n",
        "    print(\"Validation Loss : {}\".format(avg_val_loss))\n",
        "\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"This epoch took: {:}\".format(training_time))\n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('roc_auc score : ', roc_auc_score(y_true, y_pred))\n",
        "    print('F1 score : ', f1_score(y_true, y_pred))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model4\n",
        "\n",
        "print(\"-----------------------------------------------------------------------\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print('Best Accuracy : ', best_accuracy)\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFTiuhOqbzqw"
      },
      "source": [
        "#(4) Bert BiGRU With Freeze Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtybXo72bzWh"
      },
      "outputs": [],
      "source": [
        "# class BertGRUClassifierWithFreeze(nn.Module):\n",
        "#     def __init__(self, model_tune):\n",
        "#         super().__init__()\n",
        "#         self.bert = model_tune.bert\n",
        "#         self.gru = nn.LSTM(input_size = 768,\n",
        "#                             hidden_size = 768,\n",
        "#                             num_layers = 2,\n",
        "#                             batch_first = True,\n",
        "#                             bidirectional = True)\n",
        "#         self.classifier = nn.Linear(768 * 2, 2)\n",
        "#         self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "#     def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "#         bert_output = self.bert(input_ids = input_ids,\n",
        "#                                 attention_mask = attention_mask,\n",
        "#                                 token_type_ids = token_type_ids)\n",
        "#         out, _ = self.gru(bert_output[0])\n",
        "#         logits = self.classifier(out[:, 1, :])\n",
        "#         return self.softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNUo2MrqcP7S"
      },
      "outputs": [],
      "source": [
        "model5 = BertGRUClassifierWithFreeze(best_model).cuda()\n",
        "for param in model5.bert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "epochs = 3\n",
        "learning_rate = 10e-5\n",
        "optimizer = AdamW(model5.parameters(), lr = learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzpdVuD-cW4M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "best_accuracy = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('Epoch {}'.format(epoch_i + 1))\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "    model5.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        model5.zero_grad()\n",
        "        out = model5(input_ids=input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model5.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = torch.argmax(out, dim=1)\n",
        "        total_train_accuracy += torch.sum(pred == labels).item()\n",
        "\n",
        "    avg_train_accuracy = total_train_accuracy / len(train_dataloader.dataset)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_accuracy)\n",
        "\n",
        "    print(\"Training Accuracy : {}\".format(avg_train_accuracy))\n",
        "    print(\"Training Loss : {}\".format(avg_train_loss))\n",
        "\n",
        "    model5.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        input_ids = batch[0].to(device)\n",
        "        input_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model5(input_ids=input_ids, attention_mask=input_mask, token_type_ids=None)\n",
        "        loss = criterion(out, labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        pred = torch.argmax(out , dim = 1)\n",
        "        total_eval_accuracy += torch.sum(pred == labels).item()\n",
        "        y_true.append(labels.flatten())\n",
        "        y_pred.append(pred.flatten())\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader.dataset)\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader.dataset)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(avg_val_accuracy)\n",
        "\n",
        "    print(\"Validation Accuracy : {}\".format(avg_val_accuracy))\n",
        "    print(\"Validation Loss : {}\".format(avg_val_loss))\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"This epoch took: {}\".format(training_time))\n",
        "    print('------------------------------------------------------')\n",
        "    y_true = torch.cat(y_true).tolist()\n",
        "    y_pred = torch.cat(y_pred).tolist()\n",
        "    print('roc_auc score : ', roc_auc_score(y_true, y_pred))\n",
        "    print('F1 score : ', f1_score(y_true, y_pred))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Train Accur.': avg_train_accuracy,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if avg_val_accuracy > best_accuracy:\n",
        "        best_accuracy = avg_val_accuracy\n",
        "        best_model = model5\n",
        "\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(\"Total time {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
        "print('best acc:', best_accuracy)\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}