{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OGcTuijcQS3"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9ml8MVMzeRG",
        "outputId": "0cd9d5d4-8452-4b4a-9809-5ee154f2bbac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_After_Augment/ After Augmentation : 0\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/ Before Augmentation : 0\n",
            "-----------------------------------------------\n",
            "Number of files in /content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/ : 0\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive' , force_remount=True)\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# file_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/'\n",
        "# file1_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/Cats'\n",
        "# file2_path = '/content/drive/MyDrive/NNDL_HW2/HW2_Dataset/Train/Dogs'\n",
        "\n",
        "train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/'\n",
        "train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/'\n",
        "\n",
        "all_test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "# test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augment/HW2_Dataset/Test/'\n",
        "test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "files1 = [f for f in os.listdir(train_path) if os.path.isfile(os.path.join(train_path, f))]\n",
        "files2 = [f for f in os.listdir(test_path) if os.path.isfile(os.path.join(test_path, f))]\n",
        "\n",
        "test_files = [f for f in os.listdir(all_test_path) if os.path.isfile(os.path.join(all_test_path, f))]\n",
        "\n",
        "print(f\"Number of files in {train_path} After Augmentation : {len(files1)}\")\n",
        "print(f\"Number of files in {test_path} Before Augmentation : {len(files2)}\")\n",
        "print('-----------------------------------------------')\n",
        "print(f'Number of files in {test_path} : {len(test_files)}')\n",
        "\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LLdELA_ngeI"
      },
      "source": [
        "# ResNet with keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4HvVqDgzSV5",
        "outputId": "0f613834-63d8-44a4-9fe5-0ec29f3586ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Found 1685 images belonging to 2 classes.\n",
            "Found 421 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "53/53 [==============================] - 571s 11s/step - loss: 58.4494 - accuracy: 0.5187 - val_loss: 48.3054 - val_accuracy: 0.4988\n",
            "Epoch 2/10\n",
            "53/53 [==============================] - 480s 9s/step - loss: 28.6028 - accuracy: 0.5513 - val_loss: 5.2000 - val_accuracy: 0.6936\n",
            "Epoch 3/10\n",
            "53/53 [==============================] - 472s 9s/step - loss: 11.4091 - accuracy: 0.5638 - val_loss: 11.1124 - val_accuracy: 0.5416\n",
            "Epoch 4/10\n",
            "53/53 [==============================] - 406s 8s/step - loss: 21.6067 - accuracy: 0.5418 - val_loss: 5.9293 - val_accuracy: 0.6746\n",
            "Epoch 5/10\n",
            "53/53 [==============================] - 407s 8s/step - loss: 12.2736 - accuracy: 0.5804 - val_loss: 3.6995 - val_accuracy: 0.6936\n",
            "Epoch 6/10\n",
            "53/53 [==============================] - 468s 9s/step - loss: 15.9047 - accuracy: 0.5537 - val_loss: 14.8687 - val_accuracy: 0.5463\n",
            "Epoch 7/10\n",
            "53/53 [==============================] - 467s 9s/step - loss: 19.6141 - accuracy: 0.5573 - val_loss: 55.9811 - val_accuracy: 0.4988\n",
            "Epoch 8/10\n",
            "53/53 [==============================] - 405s 8s/step - loss: 15.6457 - accuracy: 0.5881 - val_loss: 6.6694 - val_accuracy: 0.6152\n",
            "Epoch 9/10\n",
            "53/53 [==============================] - 404s 8s/step - loss: 24.2949 - accuracy: 0.5513 - val_loss: 19.3399 - val_accuracy: 0.5273\n",
            "Epoch 10/10\n",
            "53/53 [==============================] - 406s 8s/step - loss: 7.2802 - accuracy: 0.6315 - val_loss: 18.4322 - val_accuracy: 0.5036\n",
            "Found 100 images belonging to 2 classes.\n",
            "4/4 [==============================] - 42s 11s/step - loss: 20.4136 - accuracy: 0.5100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[20.413557052612305, 0.5099999904632568]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define hyperparameters\n",
        "initial_lr = 0.001\n",
        "lr_decay_rate = 0.1\n",
        "momentum = 0.9\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Replace the FC layers with new ones for binary classification (dog vs cat)\n",
        "x = Flatten()(base_model.output)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Freeze all layers except the new FC layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "opt = SGD(learning_rate=initial_lr, momentum=momentum)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Data augmentation for training images\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   validation_split=0.2)  # Splitting the dataset into training and validation\n",
        "\n",
        "all_train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/'\n",
        "train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/Train'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_path,\n",
        "                                                    target_size=(224, 224),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    subset='training')  # Training set\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(train_path,\n",
        "                                                         target_size=(224, 224),\n",
        "                                                         batch_size=batch_size,\n",
        "                                                         class_mode='categorical',\n",
        "                                                         subset='validation')  # Validation set\n",
        "\n",
        "# Train the model with training and validation data\n",
        "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "\n",
        "# Unfreeze the last CONV block\n",
        "for layer in model.layers[:15]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Recompile the model to apply the changes\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "all_test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "# Test data generator for evaluation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(all_test_path ,\n",
        "                                                  target_size=(224, 224),\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical')\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(test_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhIz3DRdn3nd"
      },
      "source": [
        "# ResNet in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "lWqwOuFSn6Hq",
        "outputId": "0aea0f6c-aa71-4454-cd7d-dbd3f94f2a37"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/train'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3873372f7616>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-3873372f7616>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mimage_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/train'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "# Define hyperparameters\n",
        "initial_lr = 0.001\n",
        "momentum = 0.9\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # Replace the FC layer for binary classification\n",
        "\n",
        "# Freeze all layers except the new FC layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc.requires_grad = True\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=momentum)\n",
        "\n",
        "# Data augmentation and loading\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/'\n",
        "image_datasets = {x: datasets.ImageFolder(data_dir + x, data_transforms[x]) for x in ['train', 'test']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True) for x in ['train', 'test']}\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in dataloaders['train']:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print('Test Accuracy: {:.2f}%'.format(100 * accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAv7PnITACk3",
        "outputId": "2902122d-e876-4079-eddd-62305e42b7b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Loss: 0.0\n",
            "Epoch 2/50, Loss: 0.0\n",
            "Epoch 3/50, Loss: 0.0\n",
            "Epoch 4/50, Loss: 0.0\n",
            "Epoch 5/50, Loss: 14.829227447509766\n",
            "Epoch 6/50, Loss: 1.4901160305669237e-08\n",
            "Epoch 7/50, Loss: 0.0\n",
            "Epoch 8/50, Loss: 0.0\n",
            "Epoch 9/50, Loss: 0.0\n",
            "Epoch 10/50, Loss: 0.10322771221399307\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define paths\n",
        "all_train_path = '/content/drive/MyDrive/NNDL_HW2_After_Augment/Train/'\n",
        "all_test_path = '/content/drive/MyDrive/NNDL_HW2_Before_Augmentation/HW2_Dataset/Test/'\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all CONV layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace FC layers with new FC layer for Cats and Dogs classification\n",
        "num_classes = 2\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
        "\n",
        "# Load train and test datasets\n",
        "train_dataset = ImageFolder(all_train_path, transform=transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor()\n",
        "]))\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "test_dataset = ImageFolder(all_test_path, transform=transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor()\n",
        "]))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Train the network\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Unfreeze the last CONV block\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Fine-tune the network\n",
        "num_epochs_finetune = 50\n",
        "for epoch in range(num_epochs_finetune):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs_finetune}, Loss: {loss.item()} , Accuracy : {model.accuracy()}')\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Accuracy on test set: {100 * accuracy}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}